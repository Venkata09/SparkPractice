

The following description of the function of the spark1.4.1dataframe class description.

Only for spark version1.4.1

DataFrame function

Action action

1, collect (), the return value is an array, returns dataframe all rows

2, collectAsList () returns a value is a Java array, dataframe set all rows returned

3, count () returns a number that represents the returned dataframe set the number of rows

4, describe (cols: String*) class that returns a mathematically form values (count, mean and StdDev, min, and max), this can pass multiple parameters, separated by commas, if the field is empty, then do not participate in operation, only the numeric type fields. For example, DF. describe ("age", "height"). Show ()

5, the first () returns the first row, type is a row type

6, head () returns the first row, type is a row type

7, head (n: Int) returns the n rows, type is a row type

8, show () returns the dataframe set the value of the default 20 lines, the return type is unit

9, show (n: Int) returns n rows, the return value type is unit

10, table (n: Int) returns the n rows, type is a row type



Basics of dataframe

1, cache () synchronous data memory

2, columns returns an array of type string, the return value is the name of all columns

3, dtypes returns a two-dimensional array of type string, the return value is all the column names and types

4, explan () implementation plans physical printing

5, explain (n: Boolean) enter value is false or true, the return value is the unit defaults to false, if true will print both logical and physical

6, isLocal return value is a Boolean type, if you allow mode is local return true otherwise false

7, persist (newlevel: StorageLevel) returns a dataframe. this. type enter the storage model types

8, printSchema () prints out the field name and type according to the tree to print

9, registerTempTable (TableName: String) return Unit, DF only inside a table, the table with the delete object delete

10, structType schema returned type, name and type of the field returned by structure type

11, toDF () returns a new type of dataframe

12, toDF (colnames: String*) parameter field returns a new dataframe type,

13, unpersist () returns a dataframe. this ... type type, remove the patterns in the data

14, unpersist (blocking: Boolean) return dataframe. this ... type type unpersist and true are false is the removal of RDD



Integrated query:

1, AGG (expers: column*) returns a dataframe type, with mathematical calculations are evaluated

df.agg(max("age"), avg("salary"))

df.groupBy().agg(max("age"), avg("salary"))

2, AGG (exprs: Map[String, String]) returns a dataframe type, map math evaluation type

df.agg(Map("age" -> "max", "salary" -> "avg"))

df.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))

3, AGG (aggExpr: (String, String), aggExprs: (String, String) *) returns a dataframe type, with mathematical calculations are evaluated

df.agg(Map("age" -> "max", "salary" -> "avg"))

df.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))

4, apply (colName: String) return column types, capture input column object

5, as (alias: String) returns a new dataframe type is an alias for the original

6, Col (colName: String) return column types, capture input column object

7, cube (col1: String, cols: String*) returns a GroupedData type, based on a certain field to summarize

8, distinct types to return back to a dataframe

9, drop (col: Column) remove a column returns the type dataframe

10, dropDuplicates (colNames: Array[String]) remove the same column returns a dataframe

11, except (other: DataFrame) returns a dataframe returned do not exist in other collections in the current collection

12, explode[A, b] (inputColumn: String, outputColumn: String) (f (a) ⇒ TraversableOnce[B]) (implicit arg0: Scala. reflect. API. JavaUniverse. TypeTag[B]), the return value is the dataframe type, the To split a field more lines

df.explode("name","names") {name :String=> name.split(" ")}.show();

Name fields are split according to the blank space, split on the names of the fields

13, filter (conditionExpr: String): Brush selection, data, returns DF dataframe. filter ("age> 10"). Show (); df.filter(df("age")>10).show(); df.where(df("age")>10).show(); Can

14, groupBy (col1: String, cols: String*) according to a writing segment to the summary returns DF groupedate. groupBy ("age"). AGG (Map (the "age"-> "count")). Show (); DF. groupBy ("age"). AVG (.) show ();

15, Intersect (other: DataFrame) returns a dataframe, 2 elements of dataframe

16、 join(right: DataFrame, joinExprs: Column, joinType: String)

A dataframe is associated, the second condition that is associated, third associated types: inner, outer, left_outer, right_outer, leftsemi

df.join(ds,df("name")===ds("name") and df("age")===ds("age"),"outer").show();

17, limit (n: Int) returns to dataframe type n data

18, NA: DataFrameNaFunctions, you can call the dataframenafunctions Ribbon filter DF. na. drop (). Show (); Delete empty rows

19, orderBy (sortExprs: Column*) sort ALISE

20, select (cols: String*) dataframe brush optional DF. Select ($ "colA", $ "colB" + 1)

21, selectExpr (exprs: String*) do the field brush selected DF. selectExpr ("name", "name as names", "upper (name)", "age+1"). Show ();

22, sort (sortExprs: Column*) sort DF. sort (DF ("age"). DESC). Show (); The default is ASC

23, unionAll (other: Dataframe) with DF. unionAll (DS). Show ();

24, withColumnRenamed (existingName: String, newName: String) changes list DF. withColumnRenamed ("name", "names"). Show ();

25, withColumn (colName: String, col: Column) add a DF. withColumn ("AA", DF ("name")). Show ();
Original
6、 head() 返回第一行 ，类型是row类型



_________________________________________________________________________________________________________________










Introduction to SparkSQL

SparkSQL, formerly known as Shark, familiarity with RDBMS quick start but does not understand the MapReduce staff tools

Hive came into being, it was the only run on Hadoop SQL-on-Hadoop tool.

But MapReduce computation processes in a large number of disks drop process consumes a large amount of I/O, reduced efficiency,

In order to improve the efficiency of SQL-on-Hadoop, a large number of SQL-on-Hadoop tool, which is more prominent is: MapR Drill; Cloudera Impala; Shark

Spark Shark is the Berkeley Laboratory one of the components of the ecological environment,

It modifies the right corner as shown in the figure below the memory management, physical planning, implementation of the three modules, and allows it to run on the Spark engine, making SQL queries was 10-100 times increase.



However, with the development of Spark, Spark for ambitious teams,

Shark too much reliance on Hive (Hive parsers, query optimization, and so on), constraining the Spark One Stack of established policy to Rule Them All, constraining the Spark components integrate with each other,

Presented SparkSQL project.

SparkSQL abandoned the original Shark code to gain some benefits of the Shark,

* If the memory storage (In-Memory Columnar Storage), Hive compatibility,

Redevelopment SparkSQL code as free from dependence on the Hive, SparkSQL in terms of data compatibility, performance optimization, components, extensions are a great convenience. *

Advantages of SparkSQL

1. data compatibility

SparkSQL can handle all data storage media and a variety of formats,

Now not only compatible with the Hive, you can obtain data from the RDD, parquet, and JSON files,

Future versions and even support for RDBMS data, and Cassandra NOSQL data;

SparkSQL can simultaneously supports more data types by extension, such as Kudo.

(2) computing power

Because SparkSQL Spark-based high performance computing engines, leading to the computing power of their data warehouse level with massive increases (especially in the version 2.0 of tungsten plans matured), the upgrade includes two aspects:

1) speed

Spark the powerful computing capabilities in order to ensure, enhance calculation speed.

2) computational complexity

Since SparkSQL can be directly manipulating DataFrame,

So that the data warehouse data directly using complex algorithms and algorithm library (such as machine learning, graph, etc)

So you can perform complex going up to value of data mining.

SparkSQL future

1, SparkSQL is a data warehouse engine as an engine for data mining, scientific computing will be data and analysis engine

2, DataFrame can Spark (SQL) dominance of large data calculation engine

3, traditional databases for real-time transactional analysis has an advantage: for example bank transfers.

4, data solutions, prototypes of the future: the Hive provides an inexpensive means of data warehousing, SparkSQL responsible for the high speed calculations, DataFrame responsible for complex data mining

DataFrame profile

(1) understanding

Spark DataFrame can be simply understood as a distributed two-dimension table, which also means that each column has a name and type,

Which means SparkSQL data based on each column of metadata for more fine-grained analysis, rather than as in the previous analysis when RDD kind of coarse particle size analysis.

Also means that SparkSQL performance optimization based on DataFrame can be made more efficient.



A: Spark SQL and DataFrame



1. Spark but Spark Core SQL is the other's largest and most popular components.

A) can handle a variety of storage media, and various formats of data;

Users can extend the functionality of Spark SQL to support additional types of data (such as Kudu).

B) Spark computing power of SQL data warehouse to new heights.

Not only is unrivaled speed (especially in Tungsten mature will be more formidable,

Spark SQL at least an order of magnitude faster than the shark and Shark at least an order of magnitude faster than the Hive);

More importantly, the computational complexity of a data warehouse to a history new high

(The DataFrame can Spark follow-up launch SQL data warehouse directly using machine learning,

Calculation of complex algorithms such as depth to complex data warehousing data mining);

C) Spark SQL (DataFrame and the DataSet) not only as an engine for data warehouse and data mining engine,

More importantly Spark SQL is a scientific computing and data analysis engines!!!

D) later DataFrame Spark (SQL) to become the largest data engine technology to achieve hegemony (especially under the strong support of Tungsten)!

E) Hive+Spark SQL+DataFrame, the vast majority of companies in the past, are the solution.

I: Hive cheaper data warehousing

II: Spark SQL responsible for high speed calculation

III: DataFrame responsible for complex data mining (DataFrame is a new API, not the Spark SQL)



II: DataFrame and RDD

1, r and Python have a DataFrame, DataFrame in a Spark from a formal point of view the biggest difference is their innate is distributed;

You can simply think of Spark DataFrame is a distributed Table.

Such as: DataFrame:

Name(String)Age(Int)Tel(Long)

RDD:

Person

RDD and DataFrame fundamental differences:

A) RDD Record for the unit, Spark inside the optimization, do not anticipate Record details

So you will not be able to optimize more depth, which greatly limited the Spark SQL performance improvement!

B) DataFrame contains MetaData information for each Record,

Other words DataFrame optimization based on optimization of internal, rather than just like RDD based on optimization.



Three: Spark SQL Enterprise best practices

1, the first stage, the original code + file system is the simplest processing model, c code.

2, phase II, JavaEE+ database, the database cannot be distributed computing.

3, the third stage, the outbreak of the mobile Internet, Hive. Speed is slow.

4, the fourth stage in 2014, particularly in the second half. SparkSQL+Hive。

5, the fifth stage, SparkSQL+Hive +DataFrame.

6, the sixth stage, SparkSQL+Hive +DataFrame +DataSet (in the future).

Leading in the fifth stage, most in the third and fourth stages.


Original
Person


